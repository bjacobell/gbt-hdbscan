{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for writing a script that iteratively batches a dataset, applies HDBSCAN, pools the anomalies,\n",
    "# then does it over again.\n",
    "\n",
    "# Differences from batch_hdbscan.ipynb:\n",
    "#\n",
    "#  -  This notebook must keep track of non-anomalous clusters as well as anomalies,\n",
    "#       and we must find a decent way to match them to each other across batches.\n",
    "#\n",
    "#  -  This notebook must get rid of the day-long process in retrieve_anomalous_hits.ipynb.\n",
    "#       This will be done by assigning hits indices that are carried around with them as they\n",
    "#       get batched and shuffled.\n",
    "#\n",
    "#  -  At the end, we should have:\n",
    "#       (a) a list of anomalous hits, ideally no more than 10% of the initial dataset, which\n",
    "#           can be passed through FindEvent for interesting results\n",
    "#       (b) a list of RFI classes, which can be plotted in PDF form to demonstrate the efficacy\n",
    "#           of the clustering\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, norm, mode\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "import psutil\n",
    "import shutil\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "    freqs = data[:,0]\n",
    "    drifts = data[:,1]\n",
    "    snrs = data[:,2]\n",
    "    skews = data[:,3]\n",
    "    kurts = data[:,4]\n",
    "    sarles = data[:,5]\n",
    "    corrs = data[:,6]\n",
    "    tbws = data[:,7]\n",
    "    tskews = data[:,8]\n",
    "    tstds = data[:,9]\n",
    "    fstds = data[:,10]\n",
    "    sigbws = data[:,11]\n",
    "\n",
    "    normal_drifts = quantile_transform(drifts.reshape(len(drifts), 1), n_quantiles=100000, \n",
    "                                   output_distribution='normal', subsample=100000)\n",
    "    normal_drifts = normal_drifts.reshape(len(normal_drifts))\n",
    "\n",
    "    data_arr = np.array([np.argsort(np.argsort(freqs))/len(freqs),\n",
    "                        #0.1*(freqs-np.min(freqs))/np.max(freqs-np.min(freqs)), \n",
    "                        np.abs(normal_drifts)/np.max(np.abs(normal_drifts)), \n",
    "                        (np.log10(snrs)-np.min(np.log10(snrs)))/np.max(np.log10(snrs)-np.min(np.log10(snrs))), \n",
    "                        (skews-np.min(skews))/np.max((skews-np.min(skews))), \n",
    "                        (np.log10(kurts)-np.min(np.log10(kurts)))/np.max(np.log10(kurts)-np.min(np.log10(kurts))), \n",
    "                        sarles, \n",
    "                        corrs, \n",
    "                        (np.log10(tbws*1e6)-np.min(np.log10(tbws*1e6)))/np.max(np.log10(tbws*1e6)-np.min(np.log10(tbws*1e6))),\n",
    "                        (tskews-np.min(tskews))/np.max((tskews-np.min(tskews))),\n",
    "                        (np.log10(tstds)-np.min(np.log10(tstds))),\n",
    "                        (np.log10(fstds)-np.min(np.log10(fstds))),\n",
    "                        sigbws/np.max(sigbws)\n",
    "                        ])  ### PRE-PROCESSED FOR HDBSCAN\n",
    "\n",
    "    data_arr_unscaled = np.array([freqs, \n",
    "                        drifts, \n",
    "                        snrs, \n",
    "                        skews, \n",
    "                        kurts, \n",
    "                        sarles, \n",
    "                        corrs, \n",
    "                        tbws*1e6, # units of Hz\n",
    "                        tskews,\n",
    "                        tstds,\n",
    "                        fstds,\n",
    "                        sigbws*1e6 # units of Hz\n",
    "                        ])\n",
    "\n",
    "    return np.transpose(data_arr), np.transpose(data_arr_unscaled)\n",
    "\n",
    "def batch_hdbscan(batch_arr_scaled, nmincluster, nminsamples, eps):\n",
    "\n",
    "    hdb = HDBSCAN(\n",
    "        min_cluster_size=nmincluster, \n",
    "        min_samples=nminsamples, \n",
    "        cluster_selection_epsilon=eps, \n",
    "        #metric = 'haversine',\n",
    "        leaf_size=40,\n",
    "        n_jobs=10,\n",
    "        cluster_selection_method='eom')\n",
    "    \n",
    "    X = batch_arr_scaled\n",
    "    hdb.fit(X)\n",
    "\n",
    "    labels_list = hdb.labels_\n",
    "    centroids = hdb.centroids_\n",
    "\n",
    "    return labels_list, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_params = np.load('/datax/scratch/benjb/C23_L_unique_param_array.npy', allow_pickle=True)\n",
    "hit_dats = np.load('/datax/scratch/benjb/C23_L_unique_dat_list.npy', allow_pickle=True)[:,1]\n",
    "\n",
    "for dat in np.unique(hit_dats):\n",
    "    shutil.copy(dat, '/datax/scratch/benjb/C23_L_dats_iterative/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = [len(hit_dats)]\n",
    "non_anom_centroids = [] # tracks idx and centroid for each non-anomalous hit\n",
    "\n",
    "stg_params = np.load('/datax/scratch/benjb/C23_L_full_injected_params.npy', allow_pickle=True)\n",
    "\n",
    "full_params = np.vstack((hit_params, stg_params)) # need to be preprocessed together for quantile transforms\n",
    "full_params_scaled, full_params_unscaled = preprocess(full_params)\n",
    "\n",
    "# separate injected hits again so they can be re-injected into each batch later\n",
    "hit_params_scaled_0 = full_params_scaled[:-10]\n",
    "print(f'Size hit_params_scaled = {len(hit_params_scaled_0)}')\n",
    "hit_params_unscaled_0 = full_params_unscaled[:-10]\n",
    "print(f'Size hit_params_unscaled = {len(hit_params_unscaled_0)}')\n",
    "stg_params_scaled = full_params_scaled[-10:]\n",
    "stg_params_unscaled = full_params_unscaled[-10:]\n",
    "\n",
    "hit_idxs = np.arange(len(hit_dats))\n",
    "\n",
    "round_idxs = np.copy(hit_idxs) # i.e. 'anomalous' idxs for the current round\n",
    "\n",
    "while len(hit_dats) > 0.1 * dataset_sizes[0]:\n",
    "\n",
    "    print(f'{len(hit_dats)} hits in dataset ({100*len(hit_dats)/dataset_sizes[0]}% of original).')\n",
    "\n",
    "    n_batches = len(hit_dats) // 5000\n",
    "    batch_size = len(hit_dats) // n_batches  # should be approx but probably not exactly 5k\n",
    "\n",
    "    #round_idxs = hit_idxs[keep_idxs] # these are for tracking anom hits\n",
    "\n",
    "    hit_params_scaled = hit_params_scaled_0[round_idxs]\n",
    "    hit_params_unscaled = hit_params_unscaled_0[round_idxs]\n",
    "\n",
    "    hit_batches_scaled = []\n",
    "    hit_batches_unscaled = []\n",
    "    dat_batches = []\n",
    "    idx_batches = []\n",
    "\n",
    "    # shuffle hits before batching\n",
    "    idxs = np.arange(len(hit_params_scaled))\n",
    "    np.random.shuffle(idxs) \n",
    "    hit_params_scaled_shuffled = hit_params_scaled[idxs]\n",
    "    hit_params_unscaled_shuffled = hit_params_unscaled[idxs]\n",
    "    hit_dats_shuffled = hit_dats[idxs]\n",
    "    round_idxs_shuffled = round_idxs[idxs]\n",
    "\n",
    "    # do the batching\n",
    "    for i in range(n_batches):\n",
    "\n",
    "        if i != n_batches-1:\n",
    "            batch_scaled = hit_params_scaled_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "            batch_unscaled = hit_params_unscaled_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "            hit_dats_batch = hit_dats_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "            round_idxs_batch = round_idxs_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "        else:\n",
    "            batch_scaled = hit_params_scaled_shuffled[i*batch_size:] # catch the last few entries in the final batch\n",
    "            batch_unscaled = hit_params_unscaled_shuffled[i*batch_size:]\n",
    "            hit_dats_batch = hit_dats_shuffled[i*batch_size:]\n",
    "            round_idxs_batch = round_idxs_shuffled[i*batch_size:]\n",
    "\n",
    "        hit_batches_scaled.append(batch_scaled)\n",
    "        hit_batches_unscaled.append(batch_unscaled)\n",
    "        dat_batches.append(hit_dats_batch)\n",
    "        idx_batches.append(round_idxs_batch)\n",
    "\n",
    "    # do the HDBSCANning\n",
    "    nmincluster = 4\n",
    "    nminsamples = 2\n",
    "    eps = 0.17\n",
    "\n",
    "    dat_list_for_hit_deletion = np.empty((0, 2))\n",
    "\n",
    "    for i in range(len(hit_batches_scaled)):\n",
    "        #print('-----')\n",
    "        bbb_scaled = np.vstack((hit_batches_scaled[i], stg_params_scaled))\n",
    "        labels_list, centroids = batch_hdbscan(bbb_scaled, nmincluster, nminsamples, eps)\n",
    "        dat_freq_obj = np.transpose(np.array([hit_batches_unscaled[i][:,0], dat_batches[i]], dtype='object'))\n",
    "        non_anom_labels = np.array(list(\n",
    "            set(np.unique(labels_list))-set(np.unique(np.concatenate((labels_list[-10:], [-1]))))\n",
    "            ))\n",
    "        mask = np.isin(labels_list[:-10], non_anom_labels)\n",
    "        dat_freq_obj = dat_freq_obj[mask]\n",
    "        centroids = centroids[non_anom_labels]\n",
    "\n",
    "        # track non-anomalous hits for later superclustering \n",
    "        truncated_labels_list = labels_list[:-10]\n",
    "        non_anom_labels_list = truncated_labels_list[mask]\n",
    "        centroids_list = centroids[non_anom_labels_list]\n",
    "        for j in idx_batches[i]:\n",
    "            non_anom_centroids.append([idx_batches[i][j], centroids_list[j]])\n",
    "\n",
    "        # non-anomalous hits to be deleted\n",
    "        dat_list_for_hit_deletion = np.concatenate((dat_list_for_hit_deletion, dat_freq_obj))\n",
    "\n",
    "        # delete non-anomalous hits\n",
    "        for i in range(len(dat_list_for_hit_deletion)):\n",
    "            freq = dat_list_for_hit_deletion[i][0]\n",
    "            dat_path = '/datax/scratch/benjb/C23_L_dats_iterative/'+os.path.basename(dat_list_for_hit_deletion[i][1])\n",
    "            if i%20000 == 0:\n",
    "                print(f'{i}: Removing hit at frequency {freq} from {dat_path}')\n",
    "\n",
    "            bytes_available = psutil.virtual_memory()[1]\n",
    "            if bytes_available <= 32e9:\n",
    "                print(f'Memory dangerously low: {bytes_available} bytes remaining. Breaking ...')\n",
    "                break\n",
    "\n",
    "            lines = []\n",
    "            for line in open(dat_path):\n",
    "                if not str(freq) in line:\n",
    "                    lines.append(line)\n",
    "\n",
    "            with open(dat_path, 'w') as file:\n",
    "                file.writelines(lines)\n",
    "                file.close()\n",
    "\n",
    "        # update round_idxs ### CURRENTLY NOT CORRECT!!! Need to do for each batch.\n",
    "        round_idxs = round_idxs[~mask] # recall that round_idxs gives the anomalous idxs for the current round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False  True False  True False False False  True False]\n",
      "[ True False False  True False  True False  True  True  True False  True]\n"
     ]
    }
   ],
   "source": [
    "a = [2, 3, 5, 7, 11]\n",
    "b = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "mask = np.isin(b, a)\n",
    "print(mask)\n",
    "print(~mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
